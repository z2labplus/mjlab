#!/usr/bin/env python3
"""
éº»å°†åŒºåŸŸæ£€æµ‹å™¨æ¨ç†æµ‹è¯•è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æµ‹è§†é¢‘ä¸­çš„åŒºåŸŸ
"""

import cv2
import numpy as np
from pathlib import Path
import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple
from ultralytics import YOLO
import argparse

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MahjongRegionDetector:
    """éº»å°†åŒºåŸŸæ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, conf_threshold: float = 0.5):
        self.model_path = Path(model_path)
        self.conf_threshold = conf_threshold
        
        # åŒºåŸŸåç§°æ˜ å°„
        self.region_names = ['up', 'down', 'left', 'right', 'wind']
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # å¯è§†åŒ–é…ç½®
        self.colors = {
            'up': (0, 255, 0),      # ç»¿è‰²
            'down': (0, 0, 255),    # çº¢è‰²
            'left': (255, 0, 0),    # è“è‰²
            'right': (255, 255, 0), # é’è‰²
            'wind': (255, 0, 255)   # æ´‹çº¢è‰²
        }
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.model_path}")
        
        try:
            self.model = YOLO(str(self.model_path))
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            logger.info(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold}")
            logger.info(f"   æ”¯æŒçš„åŒºåŸŸ: {self.region_names}")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_regions_in_frame(self, frame: np.ndarray) -> List[Dict]:
        """
        åœ¨å•å¸§ä¸­æ£€æµ‹åŒºåŸŸ
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        try:
            # YOLOæ¨ç†
            results = self.model(frame, conf=self.conf_threshold, verbose=False)
            
            detections = []
            
            if results and len(results) > 0:
                result = results[0]
                
                if result.boxes is not None:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confs = result.boxes.conf.cpu().numpy()
                    classes = result.boxes.cls.cpu().numpy()
                    
                    for box, conf, cls in zip(boxes, confs, classes):
                        x1, y1, x2, y2 = map(int, box)
                        class_id = int(cls)
                        
                        if class_id < len(self.region_names):
                            region_name = self.region_names[class_id]
                            
                            detection = {
                                'region': region_name,
                                'confidence': float(conf),
                                'bbox': [x1, y1, x2, y2],
                                'center': [(x1 + x2) // 2, (y1 + y2) // 2],
                                'area': (x2 - x1) * (y2 - y1)
                            }
                            detections.append(detection)
            
            return detections
            
        except Exception as e:
            logger.error(f"åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def draw_detections(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            detections: æ£€æµ‹ç»“æœ
            
        Returns:
            æ ‡æ³¨åçš„å›¾åƒ
        """
        annotated_frame = frame.copy()
        
        for det in detections:
            region = det['region']
            confidence = det['confidence']
            x1, y1, x2, y2 = det['bbox']
            
            # è·å–é¢œè‰²
            color = self.colors.get(region, (128, 128, 128))
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{region}: {confidence:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            
            # æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(annotated_frame, 
                         (x1, y1 - label_size[1] - 10), 
                         (x1 + label_size[0], y1), 
                         color, -1)
            
            # æ ‡ç­¾æ–‡å­—
            cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return annotated_frame
    
    def test_single_image(self, image_path: str, output_path: str = None) -> Dict:
        """
        æµ‹è¯•å•å¼ å›¾åƒ
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            æ£€æµ‹ç»“æœ
        """
        logger.info(f"ğŸ–¼ï¸  æµ‹è¯•å›¾åƒ: {image_path}")
        
        # è¯»å–å›¾åƒ
        frame = cv2.imread(image_path)
        if frame is None:
            logger.error(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            return {}
        
        # æ£€æµ‹åŒºåŸŸ
        detections = self.detect_regions_in_frame(frame)
        
        # ç»˜åˆ¶ç»“æœ
        annotated_frame = self.draw_detections(frame, detections)
        
        # ä¿å­˜ç»“æœ
        if output_path:
            cv2.imwrite(output_path, annotated_frame)
            logger.info(f"ç»“æœå·²ä¿å­˜: {output_path}")
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        logger.info(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªåŒºåŸŸ:")
        for det in detections:
            logger.info(f"  - {det['region']}: ç½®ä¿¡åº¦ {det['confidence']:.3f}")
        
        return {
            'image_path': image_path,
            'detections': detections,
            'annotated_frame': annotated_frame
        }
    
    def test_video(self, video_path: str, output_path: str = None, 
                   sample_interval: int = 30) -> Dict:
        """
        æµ‹è¯•è§†é¢‘
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆå¸§ï¼‰
            
        Returns:
            æ£€æµ‹ç»Ÿè®¡ç»“æœ
        """
        logger.info(f"ğŸ¬ æµ‹è¯•è§†é¢‘: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return {}
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logger.info(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps}fps, {total_frames}å¸§")
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = None
        if output_path:
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ç»Ÿè®¡ä¿¡æ¯
        frame_count = 0
        detection_stats = {region: 0 for region in self.region_names}
        all_detections = []
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æŒ‰é—´éš”é‡‡æ ·å¤„ç†
                if frame_count % sample_interval == 0:
                    # æ£€æµ‹åŒºåŸŸ
                    detections = self.detect_regions_in_frame(frame)
                    
                    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
                    for det in detections:
                        detection_stats[det['region']] += 1
                    
                    # è®°å½•æ£€æµ‹ç»“æœ
                    all_detections.append({
                        'frame': frame_count,
                        'timestamp': frame_count / fps,
                        'detections': detections
                    })
                    
                    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
                    annotated_frame = self.draw_detections(frame, detections)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if frame_count % (sample_interval * 10) == 0:
                        progress = frame_count / total_frames * 100
                        logger.info(f"å¤„ç†è¿›åº¦: {progress:.1f}%")
                else:
                    annotated_frame = frame
                
                # å†™å…¥è¾“å‡ºè§†é¢‘
                if out is not None:
                    out.write(annotated_frame)
                
                frame_count += 1
            
        finally:
            cap.release()
            if out is not None:
                out.release()
        
        logger.info("âœ… è§†é¢‘å¤„ç†å®Œæˆ")
        logger.info("ğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
        for region, count in detection_stats.items():
            logger.info(f"  {region}: {count} æ¬¡")
        
        return {
            'video_path': video_path,
            'total_frames': total_frames,
            'processed_frames': len(all_detections),
            'detection_stats': detection_stats,
            'all_detections': all_detections
        }
    
    def analyze_region_stability(self, detections_history: List[Dict]) -> Dict:
        """
        åˆ†æåŒºåŸŸæ£€æµ‹çš„ç¨³å®šæ€§
        
        Args:
            detections_history: å†å²æ£€æµ‹ç»“æœ
            
        Returns:
            ç¨³å®šæ€§åˆ†æç»“æœ
        """
        region_positions = {region: [] for region in self.region_names}
        
        # æ”¶é›†æ¯ä¸ªåŒºåŸŸçš„ä½ç½®ä¿¡æ¯
        for frame_data in detections_history:
            detected_regions = {det['region']: det['center'] for det in frame_data['detections']}
            
            for region in self.region_names:
                if region in detected_regions:
                    region_positions[region].append(detected_regions[region])
                else:
                    region_positions[region].append(None)
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        stability_analysis = {}
        
        for region, positions in region_positions.items():
            valid_positions = [pos for pos in positions if pos is not None]
            
            if len(valid_positions) > 1:
                # è®¡ç®—ä½ç½®å˜åŒ–
                position_changes = []
                for i in range(1, len(valid_positions)):
                    prev_pos = valid_positions[i-1]
                    curr_pos = valid_positions[i]
                    change = np.sqrt((curr_pos[0] - prev_pos[0])**2 + (curr_pos[1] - prev_pos[1])**2)
                    position_changes.append(change)
                
                stability_analysis[region] = {
                    'detection_rate': len(valid_positions) / len(positions),
                    'avg_position': [np.mean([pos[0] for pos in valid_positions]),
                                   np.mean([pos[1] for pos in valid_positions])],
                    'position_variance': np.var(position_changes) if position_changes else 0,
                    'max_movement': max(position_changes) if position_changes else 0
                }
            else:
                stability_analysis[region] = {
                    'detection_rate': len(valid_positions) / len(positions),
                    'avg_position': valid_positions[0] if valid_positions else None,
                    'position_variance': 0,
                    'max_movement': 0
                }
        
        return stability_analysis

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='éº»å°†åŒºåŸŸæ£€æµ‹å™¨æµ‹è¯•')
    parser.add_argument('--model', required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--source', required=True, help='è¾“å…¥æºï¼ˆå›¾ç‰‡æˆ–è§†é¢‘è·¯å¾„ï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--conf', type=float, default=0.5, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--interval', type=int, default=30, help='è§†é¢‘é‡‡æ ·é—´éš”')
    parser.add_argument('--analysis', action='store_true', help='è¿›è¡Œç¨³å®šæ€§åˆ†æ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = MahjongRegionDetector(args.model, args.conf)
    
    source_path = Path(args.source)
    
    if source_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:
        # å›¾ç‰‡æµ‹è¯•
        output_path = args.output or f"output_{source_path.stem}_result{source_path.suffix}"
        result = detector.test_single_image(str(source_path), output_path)
        
    elif source_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv']:
        # è§†é¢‘æµ‹è¯•
        output_path = args.output or f"output_{source_path.stem}_result.mp4"
        result = detector.test_video(str(source_path), output_path, args.interval)
        
        # ç¨³å®šæ€§åˆ†æ
        if args.analysis and 'all_detections' in result:
            logger.info("ğŸ“ˆ è¿›è¡Œç¨³å®šæ€§åˆ†æ...")
            stability = detector.analyze_region_stability(result['all_detections'])
            
            logger.info("ç¨³å®šæ€§åˆ†æç»“æœ:")
            for region, stats in stability.items():
                logger.info(f"  {region}:")
                logger.info(f"    æ£€æµ‹ç‡: {stats['detection_rate']:.2%}")
                logger.info(f"    ä½ç½®æ–¹å·®: {stats['position_variance']:.2f}")
                logger.info(f"    æœ€å¤§ç§»åŠ¨: {stats['max_movement']:.2f}")
    else:
        logger.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")

if __name__ == "__main__":
    main()